
You may be wondering how this thing works — and honestly, understanding how it works will make you even better at using it.

*Note: if you already know how GPT-3 (deep learning) works, feel free to skip this.*

### The basics behind GPT-3

You may have heard that GPT-3 is made up of a lot of “parameters” — each parameter is a **specialized** **number**. When you input a sentence into GPT-3 it’ll combine that sentence with all its parameters to predict what should come next.

GPT-3 has **175 billion of these parameters**, and that takes up 800 GB.

Each parameter actually looks something like this:

```
output = input * parameter
```

*Note: I am simplifying the math here a lot, we’re just trying to understand concepts here. A lot of calculus happens within GPT-3, but I’m not trying to teach you about derivatives right now lol.*

So in the case above, the “input” is the sentence we’re providing and the “parameter” is a unique number. **We gather up all the outputs from the 175 billion unique parameters, combine them, and get our final output: words that complete the sentence.**

You may be wondering how we can multiply a sentence with a bunch of numbers. Essentially, each sentence is broken down into “**tokens**” and is converted into a set of numbers.

GPT-3 has a dictionary created that maps pieces of words to numbers. For example, the word “gang” maybe be represented as a `4332` token.

And by doing this, everything is just a number to GPT-3.

It doesn’t see words, sentences, etc. **All it sees are numbers** (Matrix?). Even when it outputs something, it outputs as numbers — but, in Playground, it’ll automatically convert the tokens to English words based on its dictionary.

This is pretty wild — it means GPT-3 doesn’t understand language like we do. Instead, it understands language as a collection of numbers and the relationships those numbers have with the parameters.

When we see “I ate food and I” we know what it means because we understand the words. So, we’ll know how to complete the sentence.

When GPT-3 sees “I ate food and I” it may see it as *“40222 5332 13211 433 45455”.* It takes these inputs, **combines them with its parameters**, and outputs “*45533 2233 4543”* which completes the sentence to “I ate food that was good”.

### Parameters — the heart of GPT-3

The parameter seems like the magical thing here, and you’d be right there.

How the hell does GPT-3 know how to complete sentences?

**Well, because the parameters were “trained” on a dataset that included a shit ton of text from the entire internet** — Wikipedia, news articles, blogs, github repos, twitter, forums, all of that stuff. GPT-3 is trained on nearly 500 billion tokens from the internet. All those YouTube comments, all those movie reviews, all those product landing pages, GPT-3 has seen all of it.

So, what does it mean when we say that the parameters are “trained” on this data?

Here’s a sentence I pulled from the Wikipedia page for Naruto:

```
A powerful fox known as the Nine-Tails attacks Konoha
```

When training GPT-3, what happens is we **remove** the last word, and tell GPT-3 to predict what comes next. So for example, during training we’d say:

```
A powerful fox known as the Nine-Tails attacks
```

Now, if GPT-3 outputs “India” at the end of this sentence, what happens? Well, we need to tell GPT-3 that it was wrong! But how do we tell GPT-3 — “Hey you should have said Konoha, not India”.

What we do here is we calculate the difference between “India” and “Konoha” — remember, GPT-3 deals in numbers. So we may say, “Hey, you gave us 4333 but we needed 32213”. We calculate the difference as an “error”.

*Please note: I am simplifying this, the math that’s happening behind the scenes to calculate error is much more involved. But, we just want you to understand it conceptually!*

This error, is then used to update each parameter in the model — we tell GPT-3 that it was off from the right answer by x amount and to adjust all its parameters slightly so it’ll be more likely to output the right answer next time.

This is called “unsupervised training”, see a visual [here](https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif).

**This process goes on and on and on.**

When training, we keep giving GPT-3 **billions** of sentences to complete until it gets really really good at completing sentences. And it keeps doing this… for a long time on a lot of text.

Fun fact: it costs about $5M in GPU time to train GPT-3.

### There’s way more to this

Because GPT-3 is huge in terms of number of parameters and because the dataset used to train it is so huge, we get something magical — a model that feels like it has a real understanding of language, context, culture and factual knowledge.

**It’s a beast trained on data generated by billions of people using the internet.** And, that’s one of the main reasons GPT-3 is so so so good. The dataset used to train it was the internet which is so vast and diverse.

What I’ve explained to you above is actually the basics of something called “deep learning” — and, deep learning is the heart of GPT-3.

You probably have a lot more questions — *What’s the math behind this thing? What is a parameter exactly, and how does a parameter update? What the heck is deep learning?*

**Here’s my advice: keep messing with GPT-3 and finish this build, don’t become obsessed with how it works right now.** Once you finish this project, I recommend going [here](https://course.fast.ai/Lessons/lesson3.html) to learn more about deep learning if you’re curious.

Also — it’s totally cool if you just **don’t get how this thing works/don’t care** and just want to use it. I doubt 99% of you *deeply* understand how the circuit board in your smartphones works. Most of us don’t give a shit! We just wanna use our smartphones.

And that’s okay. You don’t need to understand it all.

As long as you understand the base concepts, we’re good to go!
